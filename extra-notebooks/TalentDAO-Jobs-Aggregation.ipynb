{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "basePath = r'D:\\GitHub\\crypto-jobs-scraper'\n",
    "getroJobsPath = os.path.join(basePath, 'node-scraper', 'data', 'getro')\n",
    "getroAggregatePath = os.path.join(basePath, 'data', 'getro', 'all_jobs.csv')\n",
    "considerJobsPath = os.path.join(basePath, 'node-scraper', 'data', 'consider')\n",
    "considerAggregatePath = os.path.join(basePath, 'data', 'consider', 'all_jobs.csv')\n",
    "linkedinJobsPath = os.path.join(basePath, 'node-scraper', 'data', 'linkedin')\n",
    "web3CareerJobsPath = os.path.join(basePath, 'data', 'web3_careers', 'page_data')\n",
    "cryptoJobsListPath = os.path.join(basePath, 'data', 'crypto_jobs_list', 'all_jobs.csv')\n",
    "crunchbaseDataPath = 'crunchbase_data.csv'\n",
    "\n",
    "def csvFolderToDataframe(path):\n",
    "    all_files = glob.glob(os.path.join(path , \"*.csv\"))\n",
    "    \n",
    "    li = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "\n",
    "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "    return frame\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90, limit=1):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :param limit: the amount of matches that will get returned, these are sorted high to low\n",
    "    :return: dataframe with boths keys and matches\n",
    "    \"\"\"\n",
    "    s = df_2[key2].tolist()\n",
    "    \n",
    "    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=limit))    \n",
    "    df_1['matches'] = m\n",
    "    \n",
    "    m2 = df_1['matches'].apply(lambda x: ', '.join([i[0] for i in x if i[1] >= threshold]))\n",
    "    df_1['matches'] = m2\n",
    "    \n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processGetro():\n",
    "    frame = csvFolderToDataframe(getroJobsPath)\n",
    "    deduplicated = (frame.drop_duplicates(subset='Job Link', keep='first', inplace=False)\n",
    "                    .drop_duplicates(subset='Getro ObjectID', keep='first', inplace=False))\n",
    "\n",
    "    header = [\"Company Name\", \"Job Link\", \"Job Location\", \"Job Title\", \"Salary Range\", \"Tags\", \"Posted Before\"]\n",
    "    deduplicated.to_csv(getroAggregatePath, columns = header, index=False)\n",
    "    return deduplicated\n",
    "    \n",
    "getro = processGetro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processConsider():\n",
    "    frame = csvFolderToDataframe(considerJobsPath)\n",
    "    deduplicated = (frame.drop_duplicates(subset='Job Link', keep='first', inplace=False)\n",
    "                    .drop_duplicates(subset='Consider JobID', keep='first', inplace=False))\n",
    "    header = [\"Company Name\", \"Job Link\", \"Job Location\", \"Job Title\", \"Salary Range\", \"Tags\", \"Posted Before\"]\n",
    "    deduplicated.to_csv(considerAggregatePath, columns = header, index=False)\n",
    "    return deduplicated\n",
    "    \n",
    "consider = processConsider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedIn = csvFolderToDataframe(path = linkedinJobsPath)\n",
    "web3Career = csvFolderToDataframe(path = web3CareerJobsPath)\n",
    "cryptoJobsList = pd.read_csv(cryptoJobsListPath, index_col=None, header=0)\n",
    "crunchbaseData = pd.read_csv(crunchbaseDataPath, index_col=None, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7470, 10) (1584, 10) (72, 7) (17458, 7) (268, 4) (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(getro.shape, consider.shape, linkedIn.shape, web3Career.shape, cryptoJobsList.shape, crunchbaseData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getroAndConsiderJobs = pd.concat([getro, consider])\n",
    "# header = [\"Company Name\", \"Job Link\", \"Job Location\", \"Job Title\", \"Salary Range\", \"Tags\", \"Posted Before\"]\n",
    "# vcBoardJobs = getroAndConsiderJobs.drop_duplicates(subset='Job Link', keep='first', inplace=False)[header]\n",
    "# vcBoardJobs.to_csv(r'D:\\GitHub\\crypto-jobs-scraper\\data\\vc-job-boards\\all_jobs.csv', columns = header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26234, 9) (25967, 9)\n"
     ]
    }
   ],
   "source": [
    "concatenatedJobs = pd.concat([vcBoardJobs, linkedIn, web3Career, cryptoJobsList])\n",
    "allJobs = concatenatedJobs.drop_duplicates(subset='Job Link', keep='first', inplace=False)\n",
    "print(concatenatedJobs.shape, allJobs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobsPerCompany = allJobs.groupby(['Company Name'])[['Company Name']].size().reset_index(name='count').sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalDf = fuzzy_merge(crunchbaseData.head(100), jobsPerCompany, 'Company Name', 'Company Name', threshold=80, limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined = finalDf.merge(jobsPerCompany, left_on='matches', right_on='Company Name', how='left').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined[joined['count']!=''].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined.to_csv('matched_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no modifications\n",
      "58 % match for Top 100\n",
      "51 % match for Top 250\n",
      "44 % match for Top 500\n",
      "32 % match for Top 1000\n",
      "lowercase + banning generic keywords\n",
      "65 % match for Top 100\n",
      "57 % match for Top 250\n",
      "48 % match for Top 500\n",
      "36 % match for Top 1000\n"
     ]
    }
   ],
   "source": [
    "def getTopNJoinResult(crunchbase, jobsPerCompany, n, key = 'Company Name'):\n",
    "    crunchbaseTopN = crunchbase.head(n)\n",
    "    result = crunchbaseTopN.merge(jobsPerCompany, left_on=key, right_on=key, how='left').fillna('')\n",
    "    return result\n",
    "\n",
    "def printCoverage(result, n,key = 'Company Name'):\n",
    "    scraped = result[result['count']!='']\n",
    "    shape = scraped.shape\n",
    "    uniqueCompaniesCount = scraped[key].nunique()\n",
    "    print(round(uniqueCompaniesCount/n*100), \"% match\", \"for Top\", n)\n",
    "\n",
    "banned = ['foundation', 'industries', 'protocol', 'foundation', 'inc.', 'llc', 'labs', 'limited', 'chain', 'inc', 'network', 'group']\n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in banned])\n",
    "    \n",
    "crunchbaseCleaned = crunchbaseData.copy()\n",
    "crunchbaseCleaned['Company Name Lowercase'] = crunchbaseCleaned['Company Name'].str.lower()\n",
    "crunchbaseCleaned['Company Name Cleaned'] = crunchbaseCleaned['Company Name Lowercase'].apply(f)\n",
    "\n",
    "jobsPerCompanyCleaned = jobsPerCompany.copy()\n",
    "jobsPerCompanyCleaned['Company Name Lowercase'] = jobsPerCompanyCleaned['Company Name'].str.lower()\n",
    "jobsPerCompanyCleaned['Company Name Cleaned'] = jobsPerCompanyCleaned['Company Name Lowercase'].apply(f)\n",
    "\n",
    "steps = [100,250,500,1000]\n",
    "\n",
    "print(\"no modifications\")\n",
    "for step in steps:\n",
    "    result = getTopNJoinResult(crunchbaseData, jobsPerCompany, step)\n",
    "    printCoverage(result, step)\n",
    "\n",
    "print(\"lowercase + banning generic keywords\")\n",
    "for step in steps:\n",
    "    result = getTopNJoinResult(crunchbaseCleaned, jobsPerCompanyCleaned, step, 'Company Name Cleaned')\n",
    "    printCoverage(result, step, 'Company Name Cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getTopNJoinResult(crunchbaseCleaned, jobsPerCompanyCleaned, 1000, 'Company Name Cleaned')\n",
    "result.to_csv('top_1000_companies.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.Series(' '.join(crunchbaseCleaned['Company Name Cleaned']).lower().split()).value_counts().to_string())\n",
    "# jobsPerCompanyCleaned[jobsPerCompanyCleaned.duplicated(['Company Name Cleaned'], keep=False)].sort_values(\"Company Name Cleaned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
